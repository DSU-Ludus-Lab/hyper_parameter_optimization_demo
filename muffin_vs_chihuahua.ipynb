{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af9ed12",
   "metadata": {},
   "source": [
    "# Fetch Data If It Doesn't Exist\n",
    "\n",
    "Data from: https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be2f00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTSoundy1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\JTSoundy1\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"samuelcortinhas/muffin-vs-chihuahua-image-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "DATA_DIR = Path(path)  # adjust if needed\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "VAL_DIR = DATA_DIR / \"test\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7b07f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0f3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 14:54:01,506] A new study created in memory with name: muffin_vs_chihuahua\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA visible devices: None\n",
      "GPU name: NVIDIA GeForce RTX 4080 Laptop GPU\n",
      "Trial 0 | Epoch 1/3 | Train Loss: 0.3229 | Val Loss: 0.1457 | Val Acc: 0.9772\n",
      "Trial 0 | Epoch 2/3 | Train Loss: 0.1427 | Val Loss: 0.0956 | Val Acc: 0.9797\n",
      "Trial 0 | Epoch 3/3 | Train Loss: 0.1044 | Val Loss: 0.0744 | Val Acc: 0.9856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 14:55:08,191] Trial 0 finished with value: 0.9856418918918919 and parameters: {'lr': 0.0005822840473861821, 'optimizer': 'SGD', 'dropout': 0.14900869821485335, 'freeze_backbone': True, 'batch_size': 32, 'momentum': 0.5742311171402458}. Best is trial 0 with value: 0.9856418918918919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 | Epoch 1/3 | Train Loss: 0.2357 | Val Loss: 0.1532 | Val Acc: 0.9417\n",
      "Trial 1 | Epoch 2/3 | Train Loss: 0.1373 | Val Loss: 0.0925 | Val Acc: 0.9704\n",
      "Trial 1 | Epoch 3/3 | Train Loss: 0.1013 | Val Loss: 0.2029 | Val Acc: 0.9299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 14:56:19,664] Trial 1 finished with value: 0.9704391891891891 and parameters: {'lr': 0.0017506599143874144, 'optimizer': 'Adam', 'dropout': 0.5913934936453981, 'freeze_backbone': False, 'batch_size': 32}. Best is trial 0 with value: 0.9856418918918919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 | Epoch 1/3 | Train Loss: 0.4197 | Val Loss: 0.1244 | Val Acc: 0.9823\n",
      "Trial 2 | Epoch 2/3 | Train Loss: 0.2121 | Val Loss: 0.0808 | Val Acc: 0.9840\n",
      "Trial 2 | Epoch 3/3 | Train Loss: 0.1784 | Val Loss: 0.0654 | Val Acc: 0.9848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 14:57:29,238] Trial 2 finished with value: 0.9847972972972973 and parameters: {'lr': 8.386029932692986e-05, 'optimizer': 'SGD', 'dropout': 0.6923864616687978, 'freeze_backbone': True, 'batch_size': 16, 'momentum': 0.8845979289076844}. Best is trial 0 with value: 0.9856418918918919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 | Epoch 1/3 | Train Loss: 0.0987 | Val Loss: 0.0173 | Val Acc: 0.9941\n",
      "Trial 3 | Epoch 2/3 | Train Loss: 0.0211 | Val Loss: 0.0097 | Val Acc: 0.9966\n",
      "Trial 3 | Epoch 3/3 | Train Loss: 0.0135 | Val Loss: 0.0130 | Val Acc: 0.9932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 14:58:38,817] Trial 3 finished with value: 0.9966216216216216 and parameters: {'lr': 4.3232468750183284e-05, 'optimizer': 'Adam', 'dropout': 0.6595227543324216, 'freeze_backbone': False, 'batch_size': 32}. Best is trial 3 with value: 0.9966216216216216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 | Epoch 1/3 | Train Loss: 0.6379 | Val Loss: 0.4294 | Val Acc: 0.8184\n",
      "Trial 4 | Epoch 2/3 | Train Loss: 0.4081 | Val Loss: 0.3152 | Val Acc: 0.8843\n",
      "Trial 4 | Epoch 3/3 | Train Loss: 0.3560 | Val Loss: 0.4625 | Val Acc: 0.7956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 14:59:53,563] Trial 4 finished with value: 0.8842905405405406 and parameters: {'lr': 0.005698433288967356, 'optimizer': 'Adam', 'dropout': 0.046468003681466814, 'freeze_backbone': False, 'batch_size': 16}. Best is trial 3 with value: 0.9966216216216216.\n",
      "[I 2025-11-20 15:00:52,787] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 | Epoch 1/3 | Train Loss: 0.2135 | Val Loss: 0.0365 | Val Acc: 0.9873\n",
      "Trial 6 | Epoch 2/3 | Train Loss: 0.0519 | Val Loss: 0.0244 | Val Acc: 0.9932\n",
      "Trial 6 | Epoch 3/3 | Train Loss: 0.0448 | Val Loss: 0.0187 | Val Acc: 0.9941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 15:02:07,822] Trial 6 finished with value: 0.9940878378378378 and parameters: {'lr': 0.0018701797068273907, 'optimizer': 'SGD', 'dropout': 0.6672307712145075, 'freeze_backbone': False, 'batch_size': 64, 'momentum': 0.7413821851388355}. Best is trial 3 with value: 0.9966216216216216.\n",
      "[I 2025-11-20 15:03:07,277] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 | Epoch 1/3 | Train Loss: 0.1503 | Val Loss: 0.0298 | Val Acc: 0.9916\n",
      "Trial 8 | Epoch 2/3 | Train Loss: 0.0408 | Val Loss: 0.0214 | Val Acc: 0.9949\n",
      "Trial 8 | Epoch 3/3 | Train Loss: 0.0262 | Val Loss: 0.0159 | Val Acc: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 15:06:15,431] Trial 8 finished with value: 0.9966216216216216 and parameters: {'lr': 0.00018539214322044838, 'optimizer': 'SGD', 'dropout': 0.18401051206636354, 'freeze_backbone': False, 'batch_size': 32, 'momentum': 0.9629030521687365}. Best is trial 3 with value: 0.9966216216216216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 | Epoch 1/3 | Train Loss: 0.0867 | Val Loss: 0.0242 | Val Acc: 0.9916\n",
      "Trial 9 | Epoch 2/3 | Train Loss: 0.0358 | Val Loss: 0.0121 | Val Acc: 0.9975\n",
      "Trial 9 | Epoch 3/3 | Train Loss: 0.0379 | Val Loss: 0.0352 | Val Acc: 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 15:07:56,125] Trial 9 finished with value: 0.9974662162162162 and parameters: {'lr': 0.00021730902782433924, 'optimizer': 'Adam', 'dropout': 0.6574580585651273, 'freeze_backbone': False, 'batch_size': 16}. Best is trial 9 with value: 0.9974662162162162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  10\n",
      "Best trial:\n",
      "  Value (Val Accuracy): 0.9975\n",
      "  Params:\n",
      "    lr: 0.00021730902782433924\n",
      "    optimizer: Adam\n",
      "    dropout: 0.6574580585651273\n",
      "    freeze_backbone: False\n",
      "    batch_size: 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import optuna\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"CUDA visible devices:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"None\"))\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "torch.backends.cudnn.benchmark = True  # allow cuDNN to pick optimal conv algorithms\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "NUM_EPOCHS = 3   # keep small for demo; you can bump this up later\n",
    "BATCH_SIZE = 32  # this will be modified inside the objective if you want\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size: int) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create PyTorch dataloaders for train and validation.\n",
    "    \"\"\"\n",
    "    # Basic augmentation for train; just resize+center crop for val\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transform)\n",
    "    val_dataset = datasets.ImageFolder(root=VAL_DIR, transform=val_transform)\n",
    "\n",
    "    num_workers = os.cpu_count() // 2  # e.g. half your cores\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def build_model(dropout: float, freeze_backbone: bool) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create a ResNet18-based classifier with a tunable dropout and\n",
    "    an option to freeze the backbone.\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # Optionally freeze all backbone parameters\n",
    "    if freeze_backbone:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Replace the final classification layer:\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(in_features, NUM_CLASSES)\n",
    "    )\n",
    "\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train the model for a single epoch.\n",
    "    Returns average training loss.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on a validation set.\n",
    "    Returns (average loss, accuracy).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Optuna objective function.\n",
    "    Given a trial, sample hyperparameters, train the model briefly,\n",
    "    and return validation accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Sample hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.7)\n",
    "    freeze_backbone = trial.suggest_categorical(\"freeze_backbone\", [True, False])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # 2. Data loaders\n",
    "    train_loader, val_loader = get_data_loaders(batch_size)\n",
    "\n",
    "    # 3. Model, criterion, optimizer\n",
    "    model = build_model(dropout=dropout, freeze_backbone=freeze_backbone)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), lr=lr\n",
    "        )\n",
    "    else:  # SGD\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.5, 0.99)\n",
    "        optimizer = optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "\n",
    "    # 4. Training loop (short, on purpose)\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        # Report to Optuna so it can prune bad trials early\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        # Optional pruning:\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # For logging/debugging\n",
    "        print(\n",
    "            f\"Trial {trial.number} | \"\n",
    "            f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "\n",
    "# You can pass a direction=\"maximize\" since we want to maximize accuracy\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"muffin_vs_chihuahua\"\n",
    ")\n",
    "\n",
    "# Run a small number of trials for live demo.\n",
    "# Increase this for better performance.\n",
    "study.optimize(objective, n_trials=2, timeout=None)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"  Value (Val Accuracy): {best_trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a16dd0",
   "metadata": {},
   "source": [
    "# Optuna Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edbcf48b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_optimization_history, plot_param_importances\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# After study.optimize(...)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m fig_hist = plot_optimization_history(\u001b[43mstudy\u001b[49m)\n\u001b[32m      5\u001b[39m fig_hist.show()\n\u001b[32m      7\u001b[39m fig_importance = plot_param_importances(study)\n",
      "\u001b[31mNameError\u001b[39m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# After study.optimize(...)\n",
    "fig_hist = plot_optimization_history(study)\n",
    "fig_hist.show()\n",
    "\n",
    "fig_importance = plot_param_importances(study)\n",
    "fig_importance.show()\n",
    "\n",
    "plot_slice(study).show()\n",
    "plot_contour(study).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
